{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2a728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Libraries imported and directories configured.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Define Paths (Relative)\n",
    "INPUT_FILE = 'Input_data_CNN.csv'\n",
    "SAVED_MODELS_DIR = \"Saved_models\"\n",
    "TMP_DIR = \"tmp\"\n",
    "CNN_SAVE_PATH = os.path.join(SAVED_MODELS_DIR, \"CNN_model\")\n",
    "GBC_SAVE_PATH = os.path.join(SAVED_MODELS_DIR, \"gradient_boosting_model_best.pkl\")\n",
    "\n",
    "# Create directories if they do not exist\n",
    "if not os.path.exists(SAVED_MODELS_DIR):\n",
    "    os.makedirs(SAVED_MODELS_DIR)\n",
    "if not os.path.exists(TMP_DIR):\n",
    "    os.makedirs(TMP_DIR)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"- Libraries imported and directories configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992ea570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading and processing data for CNN...\n",
      "- CNN Data Loaded. Total samples: 12088\n",
      "  X_train shape: (9670, 6, 10, 1)\n",
      "  y_train shape: (9670, 4)\n"
     ]
    }
   ],
   "source": [
    "# --- CNN Data Constants ---\n",
    "CNN_WINDOW_SIZE = 6\n",
    "LABELS = ['enter', 'leave', 'stay', 'out']\n",
    "FEATURE_COLUMNS = [f'RSSI{i}' for i in range(1, 11)]\n",
    "TARGET_COLUMN = 'state'\n",
    "\n",
    "def load_and_process_cnn_data(csv_path):\n",
    "    \"\"\"\n",
    "    Loads data from CSV and creates sliding windows for CNN input.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"- Error: File {csv_path} not found.\")\n",
    "        return None, None\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Basic cleaning check\n",
    "    if len(df) < CNN_WINDOW_SIZE:\n",
    "        print(\"- Error: Not enough data.\")\n",
    "        return None, None\n",
    "        \n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    # Apply sliding window across the continuous dataset\n",
    "    # Note: We are not grouping by day here based on the provided file structure,\n",
    "    # but assuming a continuous stream for training.\n",
    "    for i in range(len(df) - CNN_WINDOW_SIZE + 1):\n",
    "        # Label is the state at the last row of the window\n",
    "        label_value = df.iloc[i + CNN_WINDOW_SIZE - 1][TARGET_COLUMN]\n",
    "        \n",
    "        if pd.isna(label_value) or label_value not in LABELS:\n",
    "            continue\n",
    "            \n",
    "        # Extract features (RSSI 1-10)\n",
    "        window_features = df.iloc[i : i + CNN_WINDOW_SIZE][FEATURE_COLUMNS].to_numpy()\n",
    "        \n",
    "        # Normalize: (RSSI + 100) / 100\n",
    "        normalized_window = (window_features + 100.0) / 100.0\n",
    "        \n",
    "        # Reshape for CNN: (6, 10, 1)\n",
    "        reshaped_window = normalized_window.reshape(CNN_WINDOW_SIZE, 10, 1)\n",
    "        \n",
    "        # One-hot encode label\n",
    "        label = np.zeros(len(LABELS))\n",
    "        label[LABELS.index(label_value)] = 1\n",
    "        \n",
    "        all_X.append(reshaped_window)\n",
    "        all_y.append(label)\n",
    "        \n",
    "    return np.array(all_X), np.array(all_y)\n",
    "\n",
    "# Load Data\n",
    "print(\"- Loading and processing data for CNN...\")\n",
    "X_cnn_all, y_cnn_all = load_and_process_cnn_data(INPUT_FILE)\n",
    "\n",
    "# Split into Train/Test for CNN Training\n",
    "# Using random split as per original code logic for CNN training\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
    "    X_cnn_all, y_cnn_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"- CNN Data Loaded. Total samples: {len(X_cnn_all)}\")\n",
    "print(f\"  X_train shape: {X_train_cnn.shape}\")\n",
    "print(f\"  y_train shape: {y_train_cnn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0704758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- CNN Model initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- CNN Hyperparameters ---\n",
    "NN_CONN2D_1 = 64\n",
    "NN_CONN2D_2 = 128\n",
    "NN_NH_1 = 256\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 100\n",
    "SAVE_STEP_EPOCHS = 3000 # As per requirement\n",
    "DISPLAY_STEP = 100\n",
    "\n",
    "class MyCNNModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyCNNModel, self).__init__()\n",
    "        self.conv1 = Conv2D(NN_CONN2D_1, 3, activation='relu', padding='same')\n",
    "        self.pool1 = MaxPooling2D(2)\n",
    "        self.conv2 = Conv2D(NN_CONN2D_2, 2, activation='relu')\n",
    "        self.pool2 = MaxPooling2D(1)\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(NN_NH_1, activation='relu')\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.d2 = Dense(len(LABELS))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        return self.d2(x)\n",
    "\n",
    "# Initialize Model\n",
    "cnn_model = MyCNNModel()\n",
    "# Build to print summary\n",
    "cnn_model.build(input_shape=(None, CNN_WINDOW_SIZE, 10, 1))\n",
    "print(\"- CNN Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6c2832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Starting CNN Training for 3000 epochs...\n",
      "  Epoch 100, Loss: 0.4786, Accuracy: 83.38%\n",
      "  Epoch 200, Loss: 0.3677, Accuracy: 87.30%\n",
      "  Epoch 300, Loss: 0.2896, Accuracy: 90.20%\n",
      "  Epoch 400, Loss: 0.2450, Accuracy: 91.91%\n",
      "  Epoch 500, Loss: 0.2208, Accuracy: 92.54%\n",
      "  Epoch 600, Loss: 0.2088, Accuracy: 93.01%\n",
      "  Epoch 700, Loss: 0.2029, Accuracy: 93.34%\n",
      "  Epoch 800, Loss: 0.1977, Accuracy: 93.36%\n",
      "  Epoch 900, Loss: 0.1952, Accuracy: 93.40%\n",
      "  Epoch 1000, Loss: 0.1903, Accuracy: 93.52%\n",
      "  Epoch 1100, Loss: 0.1894, Accuracy: 93.60%\n",
      "  Epoch 1200, Loss: 0.1864, Accuracy: 93.72%\n",
      "  Epoch 1300, Loss: 0.1864, Accuracy: 93.67%\n",
      "  Epoch 1400, Loss: 0.1860, Accuracy: 93.67%\n",
      "  Epoch 1500, Loss: 0.1852, Accuracy: 93.69%\n",
      "  Epoch 1600, Loss: 0.1840, Accuracy: 93.78%\n",
      "  Epoch 1700, Loss: 0.1830, Accuracy: 93.80%\n",
      "  Epoch 1800, Loss: 0.1819, Accuracy: 93.83%\n",
      "  Epoch 1900, Loss: 0.1817, Accuracy: 93.86%\n",
      "  Epoch 2000, Loss: 0.1816, Accuracy: 93.80%\n",
      "  Epoch 2100, Loss: 0.1829, Accuracy: 93.79%\n",
      "  Epoch 2200, Loss: 0.1809, Accuracy: 93.78%\n",
      "  Epoch 2300, Loss: 0.1813, Accuracy: 93.78%\n",
      "  Epoch 2400, Loss: 0.1808, Accuracy: 93.84%\n",
      "  Epoch 2500, Loss: 0.1813, Accuracy: 93.81%\n",
      "  Epoch 2600, Loss: 0.1816, Accuracy: 93.80%\n",
      "  Epoch 2700, Loss: 0.1806, Accuracy: 93.78%\n",
      "  Epoch 2800, Loss: 0.1821, Accuracy: 93.75%\n",
      "  Epoch 2900, Loss: 0.1785, Accuracy: 93.88%\n",
      "  Epoch 3000, Loss: 0.1800, Accuracy: 93.88%\n",
      "- CNN Training finished in 1301.20 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- CNN Model saved to: Saved_models\\CNN_model\n"
     ]
    }
   ],
   "source": [
    "# --- Training Setup ---\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = cnn_model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, cnn_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, cnn_model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(f\"- Starting CNN Training for {SAVE_STEP_EPOCHS} epochs...\")\n",
    "total_batch = int(len(X_train_cnn) / BATCH_SIZE)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(SAVE_STEP_EPOCHS):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    # Simple shuffle\n",
    "    indices = np.arange(len(X_train_cnn))\n",
    "    np.random.shuffle(indices)\n",
    "    X_shuffled = X_train_cnn[indices]\n",
    "    y_shuffled = y_train_cnn[indices]\n",
    "    \n",
    "    for batch in range(total_batch):\n",
    "        start_idx = batch * BATCH_SIZE\n",
    "        end_idx = (batch + 1) * BATCH_SIZE\n",
    "        batch_xs = X_shuffled[start_idx:end_idx]\n",
    "        batch_ys = y_shuffled[start_idx:end_idx]\n",
    "        \n",
    "        train_step(batch_xs, batch_ys)\n",
    "        \n",
    "    if (epoch + 1) % DISPLAY_STEP == 0:\n",
    "        print(f\"  Epoch {epoch+1}, Loss: {train_loss.result():.4f}, Accuracy: {train_accuracy.result()*100:.2f}%\")\n",
    "\n",
    "print(f\"- CNN Training finished in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Save CNN Model ---\n",
    "# Saving to 'Saved_models/CNN_model'\n",
    "cnn_model.save(CNN_SAVE_PATH, save_format=\"tf\")\n",
    "print(f\"- CNN Model saved to: {CNN_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3136f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Generating input features for GBC model using trained CNN...\n",
      "- GBC Features generated.\n",
      "  X_gbc shape: (12082, 28)\n",
      "  y_gbc shape: (12082,)\n",
      "  GBC Train samples: 9665\n",
      "  GBC Test samples: 2417\n"
     ]
    }
   ],
   "source": [
    "print(\"- Generating input features for GBC model using trained CNN...\")\n",
    "\n",
    "# 1. Get CNN predictions for the ENTIRE dataset (to maintain sequence)\n",
    "# We use X_cnn_all which preserves the sliding window order from the CSV\n",
    "cnn_predictions_logits = cnn_model.predict(X_cnn_all, verbose=0)\n",
    "\n",
    "# 2. Create Sliding Windows of CNN Outputs for GBC\n",
    "# GBC requires a window of 7 consecutive CNN outputs\n",
    "GBC_WINDOW = 7\n",
    "gbc_features = []\n",
    "gbc_labels = []\n",
    "\n",
    "# Determine labels (convert one-hot back to index)\n",
    "y_indices = np.argmax(y_cnn_all, axis=1)\n",
    "\n",
    "# Loop through the CNN predictions\n",
    "for i in range(len(cnn_predictions_logits) - GBC_WINDOW + 1):\n",
    "    # Extract window of 7 CNN predictions\n",
    "    # Shape: (7, 4) -> Flatten to (28,)\n",
    "    window_preds = cnn_predictions_logits[i : i + GBC_WINDOW].flatten()\n",
    "    \n",
    "    # The label for this sequence is the label of the last item in the window (offset by GBC_WINDOW - 1)\n",
    "    # Note: y_indices[i] corresponds to the label of cnn_predictions[i]\n",
    "    # So we want the label corresponding to the end of this GBC window\n",
    "    target_label = y_indices[i + GBC_WINDOW - 1]\n",
    "    \n",
    "    gbc_features.append(window_preds)\n",
    "    gbc_labels.append(target_label)\n",
    "\n",
    "X_gbc = np.array(gbc_features)\n",
    "y_gbc = np.array(gbc_labels)\n",
    "\n",
    "print(\"- GBC Features generated.\")\n",
    "print(f\"  X_gbc shape: {X_gbc.shape}\")\n",
    "print(f\"  y_gbc shape: {y_gbc.shape}\")\n",
    "\n",
    "# Split GBC data into Train/Test\n",
    "# We split this randomly as the GBC model (unlike the input generation) does not rely on sequence during training itself\n",
    "X_train_gbc, X_test_gbc, y_train_gbc, y_test_gbc = train_test_split(\n",
    "    X_gbc, y_gbc, test_size=0.2, random_state=42, stratify=y_gbc\n",
    ")\n",
    "\n",
    "print(f\"  GBC Train samples: {len(X_train_gbc)}\")\n",
    "print(f\"  GBC Test samples: {len(X_test_gbc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af4e0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Starting GBC Training with Grid Search...\n",
      "- GBC Training finished in 1169.92 seconds.\n",
      "  Best parameters: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 86}\n"
     ]
    }
   ],
   "source": [
    "print(\"- Starting GBC Training with Grid Search...\")\n",
    "\n",
    "# --- GBC Configuration ---\n",
    "gradient_boosting = GradientBoostingClassifier()\n",
    "\n",
    "# Parameter Grid (from original code)\n",
    "param_grid = {\n",
    "    'n_estimators': [70, 80, 85, 86, 90],\n",
    "    'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.45],\n",
    "    'max_depth': [2, 3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "# Stratified K-Fold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(gradient_boosting, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "start_time_gbc = time.time()\n",
    "\n",
    "# Fit model\n",
    "grid_search.fit(X_train_gbc, y_train_gbc)\n",
    "\n",
    "print(f\"- GBC Training finished in {time.time() - start_time_gbc:.2f} seconds.\")\n",
    "print(f\"  Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d66c559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Evaluating GBC Model...\n",
      "  Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       enter       0.92      0.74      0.82       315\n",
      "       leave       0.98      0.96      0.97       410\n",
      "        stay       0.99      0.96      0.97      1113\n",
      "         out       0.83      0.98      0.90       579\n",
      "\n",
      "    accuracy                           0.94      2417\n",
      "   macro avg       0.93      0.91      0.92      2417\n",
      "weighted avg       0.94      0.94      0.93      2417\n",
      "\n",
      "- GBC Model saved to: Saved_models\\gradient_boosting_model_best.pkl\n",
      "- All tasks completed successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"- Evaluating GBC Model...\")\n",
    "\n",
    "# Predict on Test Set\n",
    "y_pred_gbc = grid_search.predict(X_test_gbc)\n",
    "\n",
    "# Classification Report\n",
    "print(\"  Gradient Boosting Classification Report:\")\n",
    "print(classification_report(y_test_gbc, y_pred_gbc, target_names=LABELS))\n",
    "\n",
    "# Save Best Model\n",
    "joblib.dump(grid_search.best_estimator_, GBC_SAVE_PATH)\n",
    "print(f\"- GBC Model saved to: {GBC_SAVE_PATH}\")\n",
    "\n",
    "print(\"- All tasks completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
