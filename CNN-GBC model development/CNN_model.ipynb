{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7812c741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\12192\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "✅ 1. Necessary libraries imported and random seeds set.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"✅ 1. Necessary libraries imported and random seeds set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b01cb4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output directory already exists at: CNN_model_output\n",
      "\n",
      "✅ 2. Configuration and hyperparameters loaded.\n",
      "   - Data file: Input_data_CNN.csv\n",
      "   - Model path: CNN_model_output\n",
      "   - Labels: ['enter', 'leave', 'stay', 'out']\n",
      "   - Epochs: 3000, Batch Size: 100\n"
     ]
    }
   ],
   "source": [
    "# --- Data and Paths (Relative) ---\n",
    "# Load input data\n",
    "DATA_CSV_PATH = 'Input_data_CNN.csv' \n",
    "# Model save path (relative)\n",
    "CKPT_PATH = \"CNN_model_output\"\n",
    "\n",
    "# --- Data Processing ---\n",
    "# Labels must match the 'state' column in your CSV\n",
    "LABELS = ['enter', 'leave', 'stay', 'out'] \n",
    "# The size of the sliding window\n",
    "WINDOW_SIZE = 6 \n",
    "# The columns to be used as features\n",
    "FEATURE_COLUMNS = ['RSSI1', 'RSSI2', 'RSSI3', 'RSSI4', 'RSSI5',\n",
    "                 'RSSI6', 'RSSI7', 'RSSI8', 'RSSI9', 'RSSI10']\n",
    "TARGET_COLUMN = 'state'\n",
    "# UPDATE: Removed 'DAY_COLUMN' as it's no longer in the CSV\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "NN_CONN2D_1 = 64      # Neurons for Conv2D Layer 1\n",
    "NN_CONN2D_2 = 128     # Neurons for Conv2D Layer 2\n",
    "NN_NH_1 = 256         # Neurons for Fully Connected Layer\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 100\n",
    "SAVE_STEP_EPOCHS = 3000 # Total epochs to train\n",
    "DISPLAY_STEP = 100    # How often to print loss/accuracy\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(CKPT_PATH):\n",
    "    os.makedirs(CKPT_PATH)\n",
    "    print(f\"Created model output directory at: {CKPT_PATH}\")\n",
    "else:\n",
    "    print(f\"Model output directory already exists at: {CKPT_PATH}\")\n",
    "\n",
    "print(\"\\n✅ 2. Configuration and hyperparameters loaded.\")\n",
    "print(f\"   - Data file: {DATA_CSV_PATH}\")\n",
    "print(f\"   - Model path: {CKPT_PATH}\")\n",
    "print(f\"   - Labels: {LABELS}\")\n",
    "print(f\"   - Epochs: {SAVE_STEP_EPOCHS}, Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da37adc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 3. Data loading and preprocessing function defined (load_and_process_data).\n",
      "   - NOTE: This version assumes a single continuous data file (no 'day' grouping).\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_data(csv_path, feature_cols, target_col, labels_list, window_size):\n",
    "    \"\"\"\n",
    "    Loads data from a single CSV and creates sliding windows across the entire file.\n",
    "    Assumes the data is a single, continuous time series.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {csv_path}\")\n",
    "        print(\"Please make sure 'Input_data_CNN.csv' is in the same directory.\")\n",
    "        return None, None\n",
    "        \n",
    "    print(f\"Loaded data with {len(df)} rows.\")\n",
    "    \n",
    "    all_X_data = []\n",
    "    all_y_data = []\n",
    "    \n",
    "    print(f\"Applying sliding window (size={window_size}) across all data...\")\n",
    "    \n",
    "    # We need enough rows to make at least one window\n",
    "    if len(df) < window_size:\n",
    "        print(\"Error: Data is shorter than window size. Cannot create samples.\")\n",
    "        return None, None\n",
    "            \n",
    "    # Apply the sliding window logic across the entire dataframe\n",
    "    for i in range(len(df) - window_size + 1):\n",
    "        # The label is taken from the *last* row of the window\n",
    "        label_value = df.iloc[i + window_size - 1][target_col]\n",
    "        \n",
    "        # Skip if label is NaN or not in our defined LABELS list\n",
    "        if pd.isna(label_value) or label_value not in labels_list:\n",
    "            continue\n",
    "            \n",
    "        # Get the window of feature data (6 rows, 10 columns)\n",
    "        window_features = df.iloc[i : i + window_size][feature_cols].to_numpy()\n",
    "        \n",
    "        # --- Preprocessing ---\n",
    "        # 1. Normalize RSSI values from [-100, 0] to [0, 1]\n",
    "        normalized_window = (window_features + 100.0) / 100.0\n",
    "        \n",
    "        # 2. Reshape for CNN: (height, width, channels) -> (6, 10, 1)\n",
    "        reshaped_window = normalized_window.reshape(window_size, len(feature_cols), 1)\n",
    "        \n",
    "        # --- One-hot encode label ---\n",
    "        label = np.zeros(len(labels_list))\n",
    "        label_index = labels_list.index(label_value)\n",
    "        label[label_index] = 1\n",
    "        \n",
    "        all_X_data.append(reshaped_window)\n",
    "        all_y_data.append(label)\n",
    "\n",
    "    if not all_X_data:\n",
    "        print(\"Error: No valid windows were created. Please check your data and configuration.\")\n",
    "        return None, None\n",
    "        \n",
    "    # Convert lists to final NumPy arrays\n",
    "    X = np.array(all_X_data)\n",
    "    Y = np.array(all_y_data)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "print(\"✅ 3. Data loading and preprocessing function defined (load_and_process_data).\")\n",
    "print(\"   - NOTE: This version assumes a single continuous data file (no 'day' grouping).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2142ca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with 12093 rows.\n",
      "Applying sliding window (size=6) across all data...\n",
      "\n",
      "Total samples: 12088\n",
      "X_train shape: (9670, 6, 10, 1)\n",
      "Y_train shape: (9670, 4)\n",
      "X_test shape:  (2418, 6, 10, 1)\n",
      "Y_test shape:  (2418, 4)\n",
      "Total training batches per epoch: 96\n",
      "\n",
      "✅ 4. Data loaded, processed, and split into training/testing sets.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and process the data\n",
    "X_data, Y_data = load_and_process_data(\n",
    "    csv_path=DATA_CSV_PATH,\n",
    "    feature_cols=FEATURE_COLUMNS,\n",
    "    target_col=TARGET_COLUMN,\n",
    "    labels_list=LABELS,\n",
    "    window_size=WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# 2. Split the processed data into training and testing sets\n",
    "if X_data is not None:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_data, Y_data, test_size=0.2, random_state=42, stratify=Y_data\n",
    "    )\n",
    "    \n",
    "    # Print shapes to verify\n",
    "    print(f\"\\nTotal samples: {len(X_data)}\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"Y_train shape: {Y_train.shape}\")\n",
    "    print(f\"X_test shape:  {X_test.shape}\")\n",
    "    print(f\"Y_test shape:  {Y_test.shape}\")\n",
    "    \n",
    "    # Calculate total batches\n",
    "    total_batch = int(len(X_train) / BATCH_SIZE)\n",
    "    print(f\"Total training batches per epoch: {total_batch}\")\n",
    "    \n",
    "    print(\"\\n✅ 4. Data loaded, processed, and split into training/testing sets.\")\n",
    "else:\n",
    "    print(\"\\n❌ 4. Data loading failed. Please check errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d522eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\12192\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\12192\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\12192\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:189: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  multiple                  0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  32896     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  multiple                  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  262400    \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 296964 (1.13 MB)\n",
      "Trainable params: 296964 (1.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "✅ 5. CNN model (MyModel) defined, built, and summary printed.\n"
     ]
    }
   ],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # First convolutional layer (64 filters, 3x3 kernel)\n",
    "        self.conv1 = Conv2D(NN_CONN2D_1, 3, activation='relu', padding='same')\n",
    "        self.pool1 = MaxPooling2D(2)  # Pooling layer\n",
    "        \n",
    "        # Second convolutional layer (128 filters, 2x2 kernel)\n",
    "        self.conv2 = Conv2D(NN_CONN2D_2, 2, activation='relu')\n",
    "        self.pool2 = MaxPooling2D(1)  # Pooling layer\n",
    "        \n",
    "        self.flatten = Flatten()  # Flatten the 3D output to 1D\n",
    "        self.d1 = Dense(NN_NH_1, activation='relu')  # Fully connected layer\n",
    "        self.dropout = Dropout(0.5)  # Dropout for regularization\n",
    "        self.d2 = Dense(len(LABELS))  # Output layer (logits)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        if training:\n",
    "            # Apply dropout only during training\n",
    "            x = self.dropout(x, training=training)\n",
    "        return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()\n",
    "\n",
    "# Build the model to see the summary (optional but recommended in notebooks)\n",
    "# Input shape is (Batch Size, Height, Width, Channels)\n",
    "# We check if X_train exists before trying to build\n",
    "if 'X_train' in locals() and len(X_train) > 0:\n",
    "    model.build(input_shape=(None, WINDOW_SIZE, len(FEATURE_COLUMNS), 1))\n",
    "    model.summary()\n",
    "    print(\"\\n✅ 5. CNN model (MyModel) defined, built, and summary printed.\")\n",
    "else:\n",
    "    print(\"❌ 5. Model defined, but cannot build or show summary because data (X_train) is not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ed01d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 6. Loss function, optimizer, metrics, and @tf.function train_step defined.\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "# from_logits=True because our model outputs raw scores (logits), not probabilities\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "\n",
    "# Define metrics to measure loss and accuracy\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "# Use @tf.function to compile the training step into a high-performance graph\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        predictions = model(images, training=True)\n",
    "        # Calculate loss\n",
    "        loss = loss_object(labels, predictions)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Apply gradients to update weights\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Update metrics\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "print(\"✅ 6. Loss function, optimizer, metrics, and @tf.function train_step defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1fd0d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 3000 epochs...\n",
      "Epoch 100, Loss: 0.4761, Accuracy: 83.10%\n",
      "Epoch 200, Loss: 0.3641, Accuracy: 87.42%\n",
      "Epoch 300, Loss: 0.2888, Accuracy: 90.10%\n",
      "Epoch 400, Loss: 0.2458, Accuracy: 91.79%\n",
      "Epoch 500, Loss: 0.2238, Accuracy: 92.33%\n",
      "Epoch 600, Loss: 0.2097, Accuracy: 92.93%\n",
      "Epoch 700, Loss: 0.2034, Accuracy: 93.17%\n",
      "Epoch 800, Loss: 0.1971, Accuracy: 93.38%\n",
      "Epoch 900, Loss: 0.1956, Accuracy: 93.40%\n",
      "Epoch 1000, Loss: 0.1901, Accuracy: 93.60%\n",
      "Epoch 1100, Loss: 0.1901, Accuracy: 93.53%\n",
      "Epoch 1200, Loss: 0.1881, Accuracy: 93.67%\n",
      "Epoch 1300, Loss: 0.1882, Accuracy: 93.61%\n",
      "Epoch 1400, Loss: 0.1862, Accuracy: 93.64%\n",
      "Epoch 1500, Loss: 0.1856, Accuracy: 93.66%\n",
      "Epoch 1600, Loss: 0.1846, Accuracy: 93.75%\n",
      "Epoch 1700, Loss: 0.1861, Accuracy: 93.65%\n",
      "Epoch 1800, Loss: 0.1843, Accuracy: 93.69%\n",
      "Epoch 1900, Loss: 0.1838, Accuracy: 93.73%\n",
      "Epoch 2000, Loss: 0.1848, Accuracy: 93.68%\n",
      "Epoch 2100, Loss: 0.1832, Accuracy: 93.72%\n",
      "Epoch 2200, Loss: 0.1851, Accuracy: 93.68%\n",
      "Epoch 2300, Loss: 0.1832, Accuracy: 93.78%\n",
      "Epoch 2400, Loss: 0.1820, Accuracy: 93.79%\n",
      "Epoch 2500, Loss: 0.1826, Accuracy: 93.74%\n",
      "Epoch 2600, Loss: 0.1832, Accuracy: 93.72%\n",
      "Epoch 2700, Loss: 0.1821, Accuracy: 93.73%\n",
      "Epoch 2800, Loss: 0.1813, Accuracy: 93.74%\n",
      "Epoch 2900, Loss: 0.1818, Accuracy: 93.73%\n",
      "Epoch 3000, Loss: 0.1809, Accuracy: 93.82%\n",
      "INFO:tensorflow:Assets written to: CNN_model_output\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CNN_model_output\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train finished! Total training time: 2100.22 seconds\n",
      "Final model saved to: CNN_model_output\\model\n",
      "\n",
      "✅ 7. Model training complete and final model saved.\n"
     ]
    }
   ],
   "source": [
    "# Only run training if data was loaded correctly\n",
    "if 'X_train' in locals() and len(X_train) > 0:\n",
    "    print(f\"Starting training for {SAVE_STEP_EPOCHS} epochs...\")\n",
    "    startTime = time.time()\n",
    "\n",
    "    # Start training loop\n",
    "    for epoch in range(SAVE_STEP_EPOCHS):\n",
    "        \n",
    "        # Reset metrics at the start of each epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        \n",
    "        # Shuffle the training data\n",
    "        indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        Y_train_shuffled = Y_train[indices]\n",
    "\n",
    "        for batch in range(total_batch):\n",
    "            # Get one batch of data\n",
    "            start_idx = batch * BATCH_SIZE\n",
    "            end_idx = (batch + 1) * BATCH_SIZE\n",
    "            batch_xs, batch_ys = X_train_shuffled[start_idx:end_idx], Y_train_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Run the training step\n",
    "            if len(batch_xs) > 0: # Avoid running on empty batch\n",
    "                train_step(batch_xs, batch_ys)\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % DISPLAY_STEP == 0 or epoch == SAVE_STEP_EPOCHS - 1:\n",
    "            template = 'Epoch {}, Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "            print(template.format(epoch + 1,\n",
    "                                 train_loss.result(),\n",
    "                                 train_accuracy.result() * 100))\n",
    "\n",
    "    # Save the final model\n",
    "    model.save(os.path.join(CKPT_PATH, \"model\"), save_format=\"tf\")\n",
    "\n",
    "    # Training finished\n",
    "    print(f\"\\nTrain finished! Total training time: {time.time() - startTime:.2f} seconds\")\n",
    "    print(f\"Final model saved to: {os.path.join(CKPT_PATH, 'model')}\")\n",
    "    print(\"\\n✅ 7. Model training complete and final model saved.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 7. Training skipped because training data is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58712a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on the test set...\n",
      "76/76 [==============================] - 0s 3ms/step\n",
      "   - Test Accuracy: 0.8544\n",
      "   - Confusion matrix calculated.\n",
      "\n",
      "✅ 8. Model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Only run evaluation if data was loaded correctly\n",
    "if 'X_test' in locals() and len(X_test) > 0:\n",
    "    print(\"Running evaluation on the test set...\")\n",
    "    \n",
    "    # Use the trained model to make predictions on the test set\n",
    "    predictions_logits = model.predict(X_test)\n",
    "\n",
    "    # To get the final class prediction, find the index of the highest score (argmax)\n",
    "    predicted_classes = np.argmax(predictions_logits, axis=1)\n",
    "    # Get the true classes from the one-hot encoded Y_test\n",
    "    true_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_classes == true_classes)\n",
    "    print(f\"   - Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    print(\"   - Confusion matrix calculated.\")\n",
    "    print(\"\\n✅ 8. Model evaluation complete.\")\n",
    "else:\n",
    "    print(\"❌ 8. Evaluation skipped because test data is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1fc5e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on the test set...\n",
      "76/76 [==============================] - 0s 3ms/step\n",
      "   - Test Accuracy: 0.8544\n",
      "   - Confusion matrix calculated.\n",
      "\n",
      "✅ 8. Model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Only run evaluation if data was loaded correctly\n",
    "if 'X_test' in locals() and len(X_test) > 0:\n",
    "    print(\"Running evaluation on the test set...\")\n",
    "    \n",
    "    # Use the trained model to make predictions on the test set\n",
    "    predictions_logits = model.predict(X_test)\n",
    "\n",
    "    # To get the final class prediction, find the index of the highest score (argmax)\n",
    "    predicted_classes = np.argmax(predictions_logits, axis=1)\n",
    "    # Get the true classes from the one-hot encoded Y_test\n",
    "    true_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_classes == true_classes)\n",
    "    print(f\"   - Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    print(\"   - Confusion matrix calculated.\")\n",
    "    print(\"\\n✅ 8. Model evaluation complete.\")\n",
    "else:\n",
    "    print(\"❌ 8. Evaluation skipped because test data is not available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
